from __future__ import annotations

import argparse
import json
import math
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple


def load_curated(index_path: Path) -> List[dict]:
    rows: List[dict] = []
    if not index_path.exists():
        return rows
    with open(index_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rows.append(json.loads(line))
            except Exception:
                continue
    # Deduplicate by id, keep last occurrence
    by_id: Dict[str, dict] = {}
    for r in rows:
        try:
            ident = f"{r.get('document')}#{int(r.get('page'))}"
        except Exception:
            continue
        by_id[ident] = r
    # Return in a stable order (by id)
    return [by_id[k] for k in sorted(by_id.keys())]


def label_histogram(rows: List[dict]) -> Dict[str, int]:
    hist: Dict[str, int] = defaultdict(int)
    for r in rows:
        lbl = r.get("label") or ""
        hist[lbl] += 1
    return dict(hist)


def group_by_doc(rows: List[dict]) -> Dict[str, List[str]]:
    by_doc: Dict[str, List[str]] = defaultdict(list)
    for r in rows:
        doc = r.get("document") or ""
        try:
            pid = f"{doc}#{int(r.get('page'))}"
        except Exception:
            continue
        by_doc[doc].append(pid)
    return by_doc


def split_docs_by_pages(by_doc: Dict[str, List[str]], ratios=(0.8, 0.1, 0.1)) -> Dict[str, List[str]]:
    # Greedy pack by page count (descending) into target capacities
    docs = sorted(by_doc.items(), key=lambda kv: len(kv[1]), reverse=True)
    total_pages = sum(len(v) for _, v in docs)
    targets = [r * total_pages for r in ratios]
    assigned_pages = [0.0, 0.0, 0.0]
    buckets: List[List[str]] = [[], [], []]
    for doc, ids in docs:
        # choose split with most remaining capacity (target - assigned)
        remain = [targets[i] - assigned_pages[i] for i in range(3)]
        idx = max(range(3), key=lambda i: remain[i])
        buckets[idx].extend(ids)
        assigned_pages[idx] += len(ids)
    return {"train": buckets[0], "val": buckets[1], "test": buckets[2]}


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--version", help="Manifest/splits version tag (e.g., v3). If omitted, prints suggestion and exits.")
    ap.add_argument("--ratio", default="0.8,0.1,0.1", help="Train,Val,Test ratios (default 0.8,0.1,0.1)")
    args = ap.parse_args()

    root = Path(__file__).resolve().parents[1]
    ds_root = root / "dataset" / "v1"
    index_path = ds_root / "index" / "v1.jsonl"
    manifests_dir = ds_root / "manifests"
    splits_dir = ds_root / "splits"
    manifests_dir.mkdir(parents=True, exist_ok=True)
    splits_dir.mkdir(parents=True, exist_ok=True)

    rows = load_curated(index_path)
    if not rows:
        raise SystemExit("No curated data found.")

    # Parse ratios
    try:
        r = tuple(float(x) for x in args.ratio.split(","))
        if len(r) != 3 or not math.isclose(sum(r), 1.0, rel_tol=1e-3):
            raise ValueError
        ratios = r
    except Exception:
        ratios = (0.8, 0.1, 0.1)

    # Build manifest data
    hist = label_histogram(rows)
    docs = group_by_doc(rows)
    created_at = datetime.utcnow().isoformat(timespec="seconds") + "Z"

    if not args.version:
        # Suggest next version and exit
        existing = sorted(manifests_dir.glob("v*.json"))
        max_n = 0
        import re
        for p in existing:
            m = re.match(r"v(\d+)\.json$", p.name)
            if m:
                max_n = max(max_n, int(m.group(1)))
        print(f"Suggested next version: v{max_n+1}")
        return

    version = args.version
    manifest = {
        "version": version,
        "created_at": created_at,
        "index_path": str(index_path.as_posix()),
        "total_pages": len(rows),
        "total_documents": len(docs),
        "unique_labels": len(hist),
        "label_histogram": hist,
        "ratios": {"train": ratios[0], "val": ratios[1], "test": ratios[2]},
        "notes": "Generated by build_manifest_splits.py",
    }

    splits = split_docs_by_pages(docs, ratios=ratios)
    manifest["splits_sizes"] = {k: len(v) for k, v in splits.items()}

    # Write outputs
    man_path = manifests_dir / f"{version}.json"
    with open(man_path, "w", encoding="utf-8") as f:
        json.dump(manifest, f, ensure_ascii=False, indent=2)
        f.write("\n")

    split_path = splits_dir / f"{version}_splits.json"
    with open(split_path, "w", encoding="utf-8") as f:
        json.dump(splits, f, ensure_ascii=False, indent=2)
        f.write("\n")

    print(f"Wrote {man_path} and {split_path}")


if __name__ == "__main__":
    main()
